How did you scrape the page?

Short answer: Httrack and a python script.
Long version: It has been a long time since I performed a similar task, I had tried some download helpers tools, but I haven't done a similar study recently. Thus, I researched and tested various software (like Parsehub). Due to scraped page did not contain the actual codes, it would also be difficult for him to parse the pages for process, so I looked for the most effective and free methods. I assume you gave other people the same task, and as a result, too many requests are sent to ClassCentral. The server is protecting itself from DDoS attacks. After some experimentation, the site was scrapped with the appropriate parameters by Httrack, fair enough. You can also find a script that can do a similar task. However, because Httrack is open source and considers many problems in advance, I used it as the primary source and completed the missing parts with my code.

How did you handle the Google Translate portion?
Since the codes in the obtained files are not the actual codes of the site but the codes that appear on the running side, I faced some challenges. Two different methods have been considered to complete the task. The first is to parse the site myself. You can find this approach code in the oldâ€”code folder. The second is to use the BeautifulSoup library. The first was translating correctly. However, since there are tags outside of html, such as javascript, ajax, etc., on the site, it was not able to extract them accurately.
For this reason, the design of the site was broken. BeautifulSoup was also parsing the tags correctly and not breaking the layout. However, sometimes it failed to translate because it missed the text in the script tags. After some preprocessing and additions, I was able to parse the site correctly with BeautifulSoup. You may find the whole code files on GitHub.
Google Cloud Translation API is employed to translate texts. Languages are parametric, so that this script can translate any language html files to any other language. Credentials are stored in the same folder with the code base in this example (not necessarily every time). I divided the code into functions for easy reading.
The translation is done using the translate_html() function, which takes a string of HTML text as input and translates all the sentences in the text from the source language to the target language using the translate_sentence() function. The translate_sentence() function uses the translate_client object, an instance of the Google Cloud Translation API client, to translate a single sentence from the source to the target language. The source and target languages are specified as arguments to the function. The translate_html() function uses the BeautifulSoup library to parse the input HTML text and find all the text nodes (i.e., the text inside HTML tags). It then checks if the parent of each text node is not a style or script tag and is not a comment. If these conditions are satisfied, it passes the text node to the translate_sentence() function to translate the sentence from the source to the target language. The translated sentence is then inserted back into the HTML text.
The process_html_file() function reads an HTML file from the disk, calls the translate_html() part to translate the HTML inside the tag, and writes the translated HTML back to the file.
